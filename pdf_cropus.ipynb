{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ob39wmU4__E"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmESyMuyXYvj"
      },
      "outputs": [],
      "source": [
        "from htmlmerger import HtmlMerger\n",
        "merger = HtmlMerger(input_directory=\".\")  # result will be in my_htmls/merged.html\n",
        "merger.merge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMoHNQN0X-a6"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# # Specify the source file and destination directory\n",
        "# source_file = '/content/drive/MyDrive/fullbook clean.html'\n",
        "# destination_directory = '.'\n",
        "\n",
        "# # Use shutil.copy2 to copy the file\n",
        "# shutil.copy2(source_file, destination_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_ntHzC04_7_"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Read the HTML file\n",
        "file_path = 'fullbook.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# for tag in soup.find_all('table'):\n",
        "#     tag.extract()\n",
        "\n",
        "\n",
        "# # Remove all img elements\n",
        "# for img_tag in soup.find_all('img'):\n",
        "#     img_tag.extract()\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s28'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s19'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s14'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s13'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s5'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s2'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s28'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# for p_tag in soup.find_all('p', class_='s35'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# for h1_tag in soup.find_all('h1'):\n",
        "#     h1_tag.decompose()\n",
        "\n",
        "# for p_tag in soup.find_all('span', class_='s2'):\n",
        "#     p_tag.extract()\n",
        "\n",
        "# # code for the variable domains of the receptor. An analysis of\n",
        "\n",
        "# styles = [\n",
        "# ' color: #231F20; font-family:Symbol, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt;'\n",
        "# ,' color: #231F20; font-family:\"Trebuchet MS\", sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt;'\n",
        "# ,' color: #231F20; font-family:&quot;Trebuchet MS&quot;, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 7.5pt;'\n",
        "# ,' color: #231F20; font-family:&quot;Trebuchet MS&quot;, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;'\n",
        "# ,' color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7.5pt;'\n",
        "# ]\n",
        "# for style_type in styles:\n",
        "#     for span_tag in soup.find_all('span', style=style_type):\n",
        "#         span_tag.extract()\n",
        "\n",
        "\n",
        "# for span_tag in soup.find_all('span', style=lambda value: value and 'color: #231F20; font-family:\"Trebuchet MS\", sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt;' in value):\n",
        "#     span_tag.extract()\n",
        "\n",
        "# pattern = re.compile(r'\\(\\s*\\.\\.\\s*fig\\s*\\.\\.\\s*\\)')\n",
        "\n",
        "\n",
        "# digit_pattern = re.compile(r'\\d')\n",
        "\n",
        "# # Remove all <h3> elements with a number in their content\n",
        "# for h3_tag in soup.find_all('h4', text=digit_pattern):\n",
        "#     h3_tag.decompose()\n",
        "# # Parse the HTML content\n",
        "\n",
        "# # Remove all text containing the specified pattern\n",
        "# for tag in soup.find_all():\n",
        "#     if tag.string:\n",
        "#         tag.string = re.sub(pattern, '', tag.string)\n",
        "\n",
        "# Save the modified content back to the same file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(str(soup))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIjFmBiV09xE"
      },
      "source": [
        "### Print the number of specific html tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLtxbdRt4_5p"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the HTML file\n",
        "file_path = 'rooks_11.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "i=0\n",
        "for span_tag in soup.find_all(\"span\",class_=\"\"):\n",
        "\n",
        "    print(span_tag.get_text())\n",
        "    i+=1\n",
        "\n",
        "print(f\" -- {i} found -- \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dILZcJ6KvcPB"
      },
      "source": [
        "### Remove all empty elements from  the root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me54Pisq4_3F",
        "outputId": "e8b32e60-f541-4772-a55a-2b98a35b3e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9139 tags removed \n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the HTML file\n",
        "file_path = 'baran.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find and remove all tags with empty text content\n",
        "i=0\n",
        "for tag in soup.find_all(True):\n",
        "    if not tag.get_text(strip=True):\n",
        "        i+=1\n",
        "        tag.decompose()\n",
        "print(f'{i} tags removed ')\n",
        "# Save the modified content back to the same file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(str(soup))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### List all types of html tags"
      ],
      "metadata": {
        "id": "gJo-AgaW82-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltF3WGA3STc7"
      },
      "outputs": [],
      "source": [
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "s=set()\n",
        "for tag in soup.find_all(True):\n",
        "    s.add(tag.name)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yBweXkrdNCL"
      },
      "source": [
        "### Extract elements 1000 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbcgphYFPfIK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "file_path = 'fullbook.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "sections={}\n",
        "# ['h3','h2', 'span', 'a', 'h4', 'b', 'p', 'i', 'h2','li','ul', 'ol']\n",
        "# ['a', 'title', 'p', 'table', 'body', 'span']\n",
        "MAX=5000\n",
        "size=0\n",
        "cnt = 0\n",
        "section = 0\n",
        "txt=''\n",
        "\n",
        "# valid_tags = ['h3','h2', 'span', 'a', 'h4', 'b', 'p', 'i', 'h2','ol','ul']\n",
        "valid_tags = ['h3','a', 'title', 'p', 'table', 'body', 'span']\n",
        "\n",
        "for tag in soup.find_all():\n",
        "    if (tag.name in valid_tags):\n",
        "\n",
        "\n",
        "        if(tag.name in ['h2','h3']):\n",
        "            cnt+=1\n",
        "\n",
        "        if(size > MAX or cnt == 1):\n",
        "\n",
        "            sections[section]=txt\n",
        "            section += 1\n",
        "            txt = ''\n",
        "            size = 0\n",
        "            cnt = 0\n",
        "            print(f\"\\n\\n============ {section} ============== \\n\")\n",
        "            #time.sleep(3)\n",
        "\n",
        "\n",
        "        tag_string = tag.get_text(strip=True,separator=' ')\n",
        "        txt += tag_string+'\\n'\n",
        "        size += len(tag_string)\n",
        "        print(tag_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcnl1WB_YJX6"
      },
      "outputs": [],
      "source": [
        "# for key, value in sections.items():\n",
        "#     sections[key] = '\\n'.join(line.strip() for line in value.splitlines() if line.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV20xZ1avm5k"
      },
      "outputs": [],
      "source": [
        "with open('sections.txt','w') as file:\n",
        "    file.write('{\\n')\n",
        "    for k,v in sections.items():\n",
        "        file.write(f'\\t \"{k}\": \"{v}\"\\n')\n",
        "    file.write('}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6EnmUan8HWq"
      },
      "outputs": [],
      "source": [
        "json_str = json.dumps(sections, ensure_ascii=False, indent=4, default=lambda x: x.__dict__)\n",
        "\n",
        "# Specify the file path\n",
        "file_path = 'sections.json'\n",
        "\n",
        "# Save the JSON-formatted string to a file\n",
        "with open(file_path, 'w', encoding='utf-8') as json_file:\n",
        "    json_file.write(json_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upmlUtQdHB3O"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the HTML file\n",
        "file_path = 'fullbook.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Convert <p><span>some text</span></p> to <p>some text</p>\n",
        "for li_tag in soup.find_all('li'):\n",
        "    p_tag = li_tag.find('p')\n",
        "    if p_tag:\n",
        "        li_tag.string = p_tag.get_text()\n",
        "        p_tag.decompose()\n",
        "\n",
        "for p_tag in soup.find_all('p'):\n",
        "    span_tag = p_tag.find('span')\n",
        "    if span_tag:\n",
        "        p_tag.string = span_tag.get_text()\n",
        "        span_tag.decompose()\n",
        "\n",
        "\n",
        "# Save the modified content back to the same file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(str(soup))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrbOeThwjndW"
      },
      "source": [
        "### remove nested elemenets inside p tag so they won't appear in the P text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVVv-kUp3puy"
      },
      "outputs": [],
      "source": [
        "# Read the HTML file\n",
        "html_content = \"\"\"\n",
        "<p> <a>link</a> <i>i element</i></p>\n",
        "\"\"\"\n",
        "file_path = 'test.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Concatenate text from all sub-elements within <p> tags and strip tags\n",
        "for tag in soup.find_all(['ol', 'li']):\n",
        "    tag.unwrap()\n",
        "\n",
        "\n",
        "# Save the modified content back to the same file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(str(soup))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc7GM60PJBIT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FisjgEEGJBF7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jC6-dT9RnAC"
      },
      "source": [
        "### QA\n",
        "Direct an non direct <br>\n",
        "Ask the maximum (do not leave any information) <br>\n",
        "Questions the map early and later information in the text <br>\n",
        "Causes leading to symptoms (from your knowledge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upi3ko6GRk7c"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Read the HTML file\n",
        "file_path = 'notes.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find all <table> tags\n",
        "# Find the specific structure and transform it\n",
        "for table_tag in soup.find_all('table'):\n",
        "    p_tag = table_tag.find('a', class_='s17')\n",
        "    if p_tag:\n",
        "        table_tag.replace_with(p_tag)\n",
        "\n",
        "\n",
        "# Save the modified content back to the same file\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(str(soup))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re5oz3sVE6tK"
      },
      "source": [
        "#### Remove images, tables and tags with specifs classes that are considred as noises (depends on the document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOWpVSAsRk4e"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "file_path = 'rooks_12.html'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# for img_tag in soup.find_all('img'):\n",
        "#     img_tag.extract()\n",
        "\n",
        "# for table in soup.find_all('table'):\n",
        "#     table.extract()\n",
        "\n",
        "# for a in soup.find_all('a'):\n",
        "#     a.extract()\n",
        "\n",
        "# Find and remove all tags with empty text content\n",
        "# i=0\n",
        "# for tag in soup.find_all(True):\n",
        "#     if not tag.get_text(strip=True):\n",
        "#         i+=1\n",
        "#         tag.decompose()\n",
        "\n",
        "\n",
        "for cls in  [3,4,5,34,36,38,40,51,53,54]:\n",
        "    for tag in soup.find_all('p',class_='s'+str(cls)):\n",
        "        tag.extract()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(str(soup))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###   `MAX` : the maximum number of charchters in a sections (7000 chars ~ 1700 tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B8uXSl_s9bLm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBnKzmzYM6_5",
        "outputId": "79f698dc-bf2b-45e6-cf24-fd95f60f9e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "304 sections\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "file_path = 'rooks_11.html'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "sections={}\n",
        "\n",
        "# Select 7000 characters (~1.5k tokens)\n",
        "MAX=7000\n",
        "size=0\n",
        "cnt = 0\n",
        "section = 0\n",
        "txt=''\n",
        "\n",
        "valid_tags = ['h1','h2','h3','h4','p']\n",
        "# valid_tags = ['h1','h2','h3','h4','title', 'p', 'body', 'span','ol','ul']\n",
        "\n",
        "\n",
        "# a:h1 a17 a18\n",
        "for tag in soup.find_all():\n",
        "    if (tag.name in valid_tags):\n",
        "\n",
        "\n",
        "        # if tag.name == 'p' and any(e in tag.get('class',[]) for e in ['s3']):\n",
        "        #     cnt+=1\n",
        "        # if(tag.name == 'a' and ('name' in tag.attrs.keys())):\n",
        "        #     cnt+=1\n",
        "        #     txt+='==>'\n",
        "\n",
        "        if tag.name == 'h4':\n",
        "            cnt+=1\n",
        "\n",
        "        if(size > MAX or cnt == 2):\n",
        "            sections[section]=txt\n",
        "            section += 1\n",
        "            txt = ''\n",
        "            size = 0\n",
        "            cnt = 0\n",
        "\n",
        "        tag_string = tag.get_text(strip=True,separator=' ')\n",
        "        txt += tag_string+'\\n'\n",
        "        size += len(tag_string)\n",
        "\n",
        "\n",
        "        #print(tag_string)\n",
        "\n",
        "print(f\"{len(sections.keys())} sections\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVxUPRuIlCbS"
      },
      "outputs": [],
      "source": [
        "print(sections[474])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO6luKN_kvWD"
      },
      "source": [
        "#### Print max and min sequence length to detect anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfSFwg_lOFUo",
        "outputId": "38416fe3-7021-40e8-ad8b-25067dc4d563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9983 6606\n"
          ]
        }
      ],
      "source": [
        "lens = [len(v) for v in sections.values()]\n",
        "print(max(lens),min(lens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdUHkjNLjMqp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(lens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGdP3sN0gejK"
      },
      "outputs": [],
      "source": [
        "for k,v in sections.items():\n",
        "    if len(v) < 1000:\n",
        "        print(f\"{k}:{len(v)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the the corpus in a json like format"
      ],
      "metadata": {
        "id": "P87cmPTP9xlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDeY74IDM69N"
      },
      "outputs": [],
      "source": [
        "with open('rooks_12.json','w') as file:\n",
        "    file.write('{\\n')\n",
        "    for k,v in sections.items():\n",
        "        file.write(f'\\t \"{k}\": \"{v}\"\\n\\n,')\n",
        "    file.write('}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAj1KkceFONS"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# json_file_path = 'your_file.json'\n",
        "\n",
        "# # Dump the dictionary to a JSON file\n",
        "# with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
        "#     json.dump(sections, json_file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1JjhSEXFOJ0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIKQZcobFOHX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Z4ZHnKK6yfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8170WZ96ycj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "path = 'skincancer2.json'\n",
        "input_file_path = path\n",
        "output_file_path = \"skincancer2\"\n",
        "\n",
        "\n",
        "with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:\n",
        "    consecutive_quotes = 0\n",
        "    for line in infile:\n",
        "        for char in line:\n",
        "            if char == '\"':\n",
        "                consecutive_quotes += 1\n",
        "                if consecutive_quotes == 4:\n",
        "                    outfile.write('\",')\n",
        "                    consecutive_quotes = 0\n",
        "                else:\n",
        "                    outfile.write(char)\n",
        "            else:\n",
        "                outfile.write(char)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4M963Svs6yZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCdbICum6yW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2nRNgej46yUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KX458HH94Sq"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pdfminer.six\n",
        "!pip install pdfplumber\n",
        "!pip install pdf2image\n",
        "!pip install Pillow\n",
        "!pip install pytesseract\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoEa6WXhGMsj"
      },
      "outputs": [],
      "source": [
        "# To read the PDF\n",
        "import PyPDF2\n",
        "# To analyze the PDF layout and extract text\n",
        "from pdfminer.high_level import extract_pages, extract_text\n",
        "from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure\n",
        "# To extract text from tables in PDF\n",
        "import pdfplumber\n",
        "# To extract the images from the PDFs\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "# To perform OCR to extract text from images\n",
        "import pytesseract\n",
        "# To remove the additional created files\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4d8R8ShGMp_"
      },
      "outputs": [],
      "source": [
        "def text_extraction(element):\n",
        "    # Extracting the text from the in-line text element\n",
        "    line_text = element.get_text()\n",
        "\n",
        "    # Find the formats of the text\n",
        "    # Initialize the list with all the formats that appeared in the line of text\n",
        "    line_formats = []\n",
        "    for text_line in element:\n",
        "        if isinstance(text_line, LTTextContainer):\n",
        "            # Iterating through each character in the line of text\n",
        "            for character in text_line:\n",
        "                if isinstance(character, LTChar):\n",
        "                    # Append the font name of the character\n",
        "                    line_formats.append(character.fontname)\n",
        "                    # Append the font size of the character\n",
        "                    line_formats.append(character.size)\n",
        "    # Find the unique font sizes and names in the line\n",
        "    format_per_line = list(set(line_formats))\n",
        "\n",
        "    # Return a tuple with the text in each line along with its format\n",
        "    return (line_text, format_per_line)\n",
        "\n",
        "\n",
        "# Create a function to crop the image elements from PDFs\n",
        "def crop_image(element, pageObj):\n",
        "    # Get the coordinates to crop the image from the PDF\n",
        "    [image_left, image_top, image_right, image_bottom] = [element.x0,element.y0,element.x1,element.y1]\n",
        "    # Crop the page using coordinates (left, bottom, right, top)\n",
        "    pageObj.mediabox.lower_left = (image_left, image_bottom)\n",
        "    pageObj.mediabox.upper_right = (image_right, image_top)\n",
        "    # Save the cropped page to a new PDF\n",
        "    cropped_pdf_writer = PyPDF2.PdfWriter()\n",
        "    cropped_pdf_writer.add_page(pageObj)\n",
        "    # Save the cropped PDF to a new file\n",
        "    with open('cropped_image.pdf', 'wb') as cropped_pdf_file:\n",
        "        cropped_pdf_writer.write(cropped_pdf_file)\n",
        "\n",
        "# Create a function to convert the PDF to images\n",
        "def convert_to_images(input_file,):\n",
        "    images = convert_from_path(input_file)\n",
        "    image = images[0]\n",
        "    output_file = \"PDF_image.png\"\n",
        "    image.save(output_file, \"PNG\")\n",
        "\n",
        "# Create a function to read text from images\n",
        "def image_to_text(image_path):\n",
        "    # Read the image\n",
        "    img = Image.open(image_path)\n",
        "    # Extract the text from the image\n",
        "    text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def extract_table(pdf_path, page_num, table_num):\n",
        "    # Open the pdf file\n",
        "    pdf = pdfplumber.open(pdf_path)\n",
        "    # Find the examined page\n",
        "    table_page = pdf.pages[page_num]\n",
        "    # Extract the appropriate table\n",
        "    table = table_page.extract_tables()[table_num]\n",
        "    return table\n",
        "\n",
        "# Convert table into the appropriate format\n",
        "def table_converter(table):\n",
        "    table_string = ''\n",
        "    # Iterate through each row of the table\n",
        "    for row_num in range(len(table)):\n",
        "        row = table[row_num]\n",
        "        # Remove the line breaker from the wrapped texts\n",
        "        cleaned_row = [item.replace('\\n', ' ') if item is not None and '\\n' in item else 'None' if item is None else item for item in row]\n",
        "        # Convert the table into a string\n",
        "        table_string+=('|'+'|'.join(cleaned_row)+'|'+'\\n')\n",
        "    # Removing the last line break\n",
        "    table_string = table_string[:-1]\n",
        "    return table_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtGnRV6KO4ge"
      },
      "source": [
        "### Adding all together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztuU2Xf-O3Zq"
      },
      "outputs": [],
      "source": [
        "# Find the PDF path\n",
        "pdf_path = 'pages.pdf'\n",
        "\n",
        "# create a PDF file object\n",
        "pdfFileObj = open(pdf_path, 'rb')\n",
        "# create a PDF reader object\n",
        "pdfReaded = PyPDF2.PdfReader(pdfFileObj)\n",
        "# Create the dictionary to extract text from each image\n",
        "text_per_page = {}\n",
        "# We extract the pages from the PDF\n",
        "for pagenum, page in enumerate(extract_pages(pdf_path)):\n",
        "\n",
        "    # Initialize the variables needed for the text extraction from the page\n",
        "    pageObj = pdfReaded.pages[pagenum]\n",
        "    page_text = []\n",
        "    line_format = []\n",
        "    text_from_images = []\n",
        "    text_from_tables = []\n",
        "    page_content = []\n",
        "    # Initialize the number of the examined tables\n",
        "    table_num = 0\n",
        "    first_element= True\n",
        "    table_extraction_flag= False\n",
        "    # Open the pdf file\n",
        "    pdf = pdfplumber.open(pdf_path)\n",
        "    # Find the examined page\n",
        "    page_tables = pdf.pages[pagenum]\n",
        "    # Find the number of tables on the page\n",
        "    tables = page_tables.find_tables()\n",
        "\n",
        "\n",
        "    # Find all the elements\n",
        "    page_elements = [(element.y1, element) for element in page._objs]\n",
        "    # Sort all the elements as they appear in the page\n",
        "    page_elements.sort(key=lambda a: a[0], reverse=True)\n",
        "\n",
        "    # Find the elements that composed a page\n",
        "    for i,component in enumerate(page_elements):\n",
        "        # Extract the position of the top side of the element in the PDF\n",
        "        pos= component[0]\n",
        "        # Extract the element of the page layout\n",
        "        element = component[1]\n",
        "\n",
        "        # Check if the element is a text element\n",
        "        if isinstance(element, LTTextContainer):\n",
        "            # Check if the text appeared in a table\n",
        "            if table_extraction_flag == False:\n",
        "                # Use the function to extract the text and format for each text element\n",
        "                (line_text, format_per_line) = text_extraction(element)\n",
        "                # Append the text of each line to the page text\n",
        "                page_text.append(line_text)\n",
        "                # Append the format for each line containing text\n",
        "                line_format.append(format_per_line)\n",
        "                page_content.append(line_text)\n",
        "            else:\n",
        "                # Omit the text that appeared in a table\n",
        "                pass\n",
        "\n",
        "        # Check the elements for images\n",
        "        if isinstance(element, LTFigure):\n",
        "            # Crop the image from the PDF\n",
        "            crop_image(element, pageObj)\n",
        "            # Convert the cropped pdf to an image\n",
        "            convert_to_images('cropped_image.pdf')\n",
        "            # Extract the text from the image\n",
        "            image_text = image_to_text('PDF_image.png')\n",
        "            text_from_images.append(image_text)\n",
        "            page_content.append(image_text)\n",
        "            # Add a placeholder in the text and format lists\n",
        "            page_text.append('image')\n",
        "            line_format.append('image')\n",
        "\n",
        "        # Check the elements for tables\n",
        "        if isinstance(element, LTRect):\n",
        "            # If the first rectangular element\n",
        "            if first_element == True and (table_num+1) <= len(tables):\n",
        "                # Find the bounding box of the table\n",
        "                lower_side = page.bbox[3] - tables[table_num].bbox[3]\n",
        "                upper_side = element.y1\n",
        "                # Extract the information from the table\n",
        "                table = extract_table(pdf_path, pagenum, table_num)\n",
        "                # Convert the table information in structured string format\n",
        "                table_string = table_converter(table)\n",
        "                # Append the table string into a list\n",
        "                text_from_tables.append(table_string)\n",
        "                page_content.append(table_string)\n",
        "                # Set the flag as True to avoid the content again\n",
        "                table_extraction_flag = True\n",
        "                # Make it another element\n",
        "                first_element = False\n",
        "                # Add a placeholder in the text and format lists\n",
        "                page_text.append('table')\n",
        "                line_format.append('table')\n",
        "\n",
        "                # Check if we already extracted the tables from the page\n",
        "                if element.y0 >= lower_side and element.y1 <= upper_side:\n",
        "                    pass\n",
        "                elif not isinstance(page_elements[i+1][1], LTRect):\n",
        "                    table_extraction_flag = False\n",
        "                    first_element = True\n",
        "                    table_num+=1\n",
        "\n",
        "\n",
        "    # Create the key of the dictionary\n",
        "    dctkey = 'Page_'+str(pagenum)\n",
        "    # Add the list of list as the value of the page key\n",
        "    text_per_page[dctkey]= [page_text, line_format, text_from_images,text_from_tables, page_content]\n",
        "\n",
        "# Closing the pdf file object\n",
        "pdfFileObj.close()\n",
        "\n",
        "# Deleting the additional files created\n",
        "# os.remove('cropped_image.pdf')\n",
        "# os.remove('PDF_image.png')\n",
        "\n",
        "# Display the content of the page\n",
        "result = ''.join(text_per_page['Page_0'][4])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvA0rJwaSVcC"
      },
      "outputs": [],
      "source": [
        "text_per_page['Page_0'][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l3CrRFMTGHH"
      },
      "outputs": [],
      "source": [
        "def page(i):\n",
        "    return ''.join(text_per_page[f'Page_{i}'][4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY_QiCuVWPln"
      },
      "outputs": [],
      "source": [
        "print(page(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufTTbIveazqc"
      },
      "outputs": [],
      "source": [
        "!pip install fitz\n",
        "!pip install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmWzsEZjaf91"
      },
      "outputs": [],
      "source": [
        "import fitz # imports the pymupdf library\n",
        "doc = fitz.open(\"pages.pdf\") # open a document\n",
        "for i,page in enumerate(doc): # iterate the document pages\n",
        "    if(i!=1):\n",
        "        continue\n",
        "    else:\n",
        "        text = page.get_text() # get plain text encoded as UTF-8\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YA4FQig3f4S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCeQ4IfC3f1u",
        "outputId": "af03fed5-ca33-4c34-ab3e-e8707c07748d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "70+13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjXoVc863fzb",
        "outputId": "5dbf18b0-d93e-4e68-bd21-aed8aa5cc895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2,\n",
              " 14,\n",
              " 26,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 74,\n",
              " 86,\n",
              " 98,\n",
              " 110,\n",
              " 122,\n",
              " 134,\n",
              " 146,\n",
              " 158,\n",
              " 170,\n",
              " 182,\n",
              " 194,\n",
              " 206,\n",
              " 218,\n",
              " 230,\n",
              " 242,\n",
              " 254,\n",
              " 266,\n",
              " 278,\n",
              " 290,\n",
              " 302,\n",
              " 314,\n",
              " 326,\n",
              " 338,\n",
              " 350,\n",
              " 362,\n",
              " 374,\n",
              " 386,\n",
              " 398,\n",
              " 410,\n",
              " 422,\n",
              " 434,\n",
              " 446,\n",
              " 458,\n",
              " 470,\n",
              " 482,\n",
              " 494,\n",
              " 506,\n",
              " 518,\n",
              " 530,\n",
              " 542,\n",
              " 554,\n",
              " 566,\n",
              " 578,\n",
              " 590,\n",
              " 602,\n",
              " 614,\n",
              " 626,\n",
              " 638,\n",
              " 650,\n",
              " 662,\n",
              " 674,\n",
              " 686,\n",
              " 698,\n",
              " 710,\n",
              " 722,\n",
              " 734,\n",
              " 746,\n",
              " 758,\n",
              " 770,\n",
              " 782,\n",
              " 794,\n",
              " 806]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ranges = list(range(1,743,11))\n",
        "for i in range(len(ranges)):\n",
        "    ranges[i] = ranges[i]+i\n",
        "ranges"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}